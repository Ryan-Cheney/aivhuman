# -*- coding: utf-8 -*-
"""FaceForensics_XceptionNet_Code.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1JCakmxJcWPO1qetanxiOOdhYuCfmZmZb
"""

!pip install Pillow

from IPython import get_ipython
from IPython.display import display
# %%
import pandas as pd
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
import torchvision
import torchvision.transforms as transforms
import torch.nn as nn
import torch.optim as optim
from torchvision import datasets
from torchvision.io import read_image
import glob
import cv2
import matplotlib.pyplot as plt
from torch.utils.data import Dataset, DataLoader
# Import Image from PIL and alias it as PILImage
from PIL import Image as PILImage
import timm
torch.manual_seed(42)
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(device)
import torchvision.models as models
import os

!pip install kaggle==1.5.12

!mkdir ~/.kaggle
!cp kaggle.json ~/.kaggle/
!chmod 600 ~/.kaggle/kaggle.json

!kaggle datasets download -d alessandrasala79/ai-vs-human-generated-dataset

"""## Dataset extracted from Face Forensics Videos. DataLoader Creation"""

class CustomDataset(Dataset):
    def __init__(self, csv_file, root_dir, transform=None, load_labels=True, data=None):  # Added data parameter
        if csv_file is not None:
            self.data = pd.read_csv(csv_file)
        else:
            self.data = data  # Use the provided data if csv_file is None
        self.root_dir = root_dir
        self.transform = transform
        self.load_labels = load_labels
        self.label_mapping = {'real': 0, 'fake': 1}

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        # Assuming the first column in your CSV contains the relative path to the image
        img_path = os.path.join(self.root_dir, self.data['file_name'].iloc[idx])  # Construct the full path

        # Use PILImage.open with error handling:
        try:
            # Use PILImage to open the image
            image = PILImage.open(img_path).convert('RGB')
        except FileNotFoundError:
            print(f"Error: File not found at path: {img_path}")
            # Create a black image using PILImage
            image = PILImage.new('RGB', (128, 128), color=(0, 0, 0))

        except Exception as e:
            print(f"Error opening image: {img_path}, Error: {e}")
            # Create a black image using PILImage
            image = PILImage.new('RGB', (128, 128), color=(0, 0, 0))

        if self.transform:
            image = self.transform(image)

        if self.load_labels:
            # Get the label and clean it if needed
            label = self.data.iloc[idx, 1]

            # Remove whitespaces or other unexpected characters
            label = label.strip() # Example: Removes leading/trailing spaces.


            # Handle unknown labels or add to label_mapping
            if label not in self.label_mapping:
                print(f"Unknown label found: {label}, Adding it to mapping with value {len(self.label_mapping)}")
                self.label_mapping[label] = len(self.label_mapping)  # Add to mapping dynamically


            # Get numerical label from the mapping
            numerical_label = self.label_mapping.get(label)

            # Print label and its corresponding numerical value
            print(f"Original label: '{label}', Numerical label: {numerical_label}")

            # Convert to tensor
            label_tensor = torch.tensor(numerical_label, dtype=torch.long)

            return image, label_tensor
        else:
            return image

train_transforms = transforms.Compose([
       transforms.Resize([128, 128]),
       transforms.ToTensor(),
   ])
test_transforms = train_transforms

"""## Training and Testing Split"""

train_dataset = CustomDataset(csv_file='train.csv', root_dir='/content', transform=train_transforms, load_labels=True)
test_dataset = CustomDataset(csv_file='test.csv', root_dir='/content', transform=test_transforms, load_labels=False)

len(train_dataset)

len(test_dataset)

"""## XceptionNet Model"""

from IPython.display import Image
# Access the first element of the 'file_name' column as a string
image_path = train_dataset.data['file_name'].iloc[2]
# Prepend the root directory to the image path
image_path = os.path.join(train_dataset.root_dir, image_path)
Image(filename=image_path)

model = timm.create_model('xception', pretrained=True, num_classes=1)
model = model.to(device)
print(model)

"""## Optimizer and Loss Function

"""

optimizer = torch.optim.Adam(model.parameters(),lr = 0.0001)
criterion = nn.BCEWithLogitsLoss()

"""## Training code"""

import torch
from torch.utils.data import Subset, DataLoader
import numpy as np

# Ensure reproducibility
np.random.seed(42)
torch.manual_seed(42)

# Generate 100 random indices from the total dataset for training
num_samples = 100
total_samples = len(train_dataset)  # Should be 80,000
train_indices = np.random.choice(total_samples, num_samples, replace=False)

# Create a subset dataset using these indices for training
train_subset = Subset(train_dataset, train_indices)

# Use this subset for training
train_data_loader = DataLoader(train_subset, batch_size=32, shuffle=True)

# ------------------ Create validation data loader ------------------

# Get remaining indices for validation
remaining_indices = np.setdiff1d(np.arange(total_samples), train_indices)

# Randomly select 100 indices from the remaining indices for validation
val_indices = np.random.choice(remaining_indices, num_samples, replace=False)

# Create a subset dataset using these indices for validation
val_subset = Subset(train_dataset, val_indices)

# Use this subset for validation
val_data_loader = DataLoader(val_subset, batch_size=32, shuffle=False)

import torch
from torch.utils.data import DataLoader
import pandas as pd
import numpy as np

# Set the model to evaluation mode
model.eval()

# Create a list to store the features and labels
all_features = []
all_labels = []


# Get features from the model, ensuring global average pooling
with torch.no_grad():
    for data in train_data_loader:
        inputs, labels = data
        inputs = inputs.to(device)
        labels = labels.to(device)

        # Extract features correctly with GAP
        features = model.forward_features(inputs.float())

        # Apply Global Average Pooling
        features = torch.nn.functional.adaptive_avg_pool2d(features, 1)

        # Flatten the features
        features = features.view(features.size(0), -1)

        # Convert to NumPy and store
        all_features.append(features.cpu().numpy())
        all_labels.append(labels.cpu().numpy())


# Concatenate all features and labels into single arrays
all_features = np.concatenate(all_features, axis=0)
all_labels = np.concatenate(all_labels, axis=0)

# Create a Pandas DataFrame
df = pd.DataFrame(all_features)
df['label'] = all_labels  # Add the labels as a column

# Save the DataFrame to a CSV file
df.to_csv('extracted_features1.csv', index=False)